{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"meme_gen.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# # This is the TPU initialization code that has to be at the beginning.\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "# strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "\n",
    "# !git clone https://github.com/TheBrainGamingSingh/BrainGamingMemes.git\n",
    "\n",
    "# ls\n",
    "\n",
    "# cd  BrainGamingMemes/\n",
    "\n",
    "# ls\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "# Import Keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Dropout, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import Sequential\n",
    "\n",
    "# Helper Functions\n",
    "def map_char_to_int(texts):\n",
    "    char_counts = {}\n",
    "    for text in texts:\n",
    "        for char in text:\n",
    "            char_counts[char] = char_counts[char] + 1 if char in char_counts else 1\n",
    "    char_counts_sorted = sorted(char_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    char_to_int = {}\n",
    "    for i, row in enumerate(char_counts_sorted):\n",
    "        char_to_int[row[0]] = i + 1\n",
    "    return char_to_int\n",
    "\n",
    "def texts_to_sequences(texts, char_to_int):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequences.append([char_to_int[char] for char in text])\n",
    "    return sequences\n",
    "\n",
    "# Config\n",
    "\n",
    "BASE_PATH = os.getcwd()\n",
    "MODEL_NAME = 'brain_gaming_memes'\n",
    "\n",
    "try: \n",
    "    os.mkdir('models')\n",
    "except FileExistsError:\n",
    "    print('models directory already exists...')\n",
    "\n",
    "MODEL_PATH = BASE_PATH + '/models/' + MODEL_NAME + '_' + datetime.datetime.today().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "os.mkdir(MODEL_PATH)\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 16\n",
    "ROWS_TO_SCAN = 100000\n",
    "NUM_EPOCHS = 48\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "MEME_IDS = [ '114585149',\n",
    "             '438680',\n",
    "             '100777631',\n",
    "             '181913649',\n",
    "             '161865971',\n",
    "             '217743513',\n",
    "             '131087935',\n",
    "             '119139145',\n",
    "             '93895088',\n",
    "             '112126428',\n",
    "             '97984',\n",
    "             '1035805',\n",
    "             '155067746',\n",
    "             '4087833',\n",
    "             '91538330',\n",
    "             '124822590',\n",
    "             '178591752',\n",
    "             '124055727',\n",
    "             '87743020',\n",
    "             '222403160',\n",
    "             '102156234',\n",
    "             '188390779',\n",
    "             '89370399',\n",
    "             '129242436']\n",
    "\n",
    "# Generate input for the model\n",
    "# Read Training Data\n",
    "MEME_IDS = [ '100777631']\n",
    "\n",
    "with open('training_data.json') as file:\n",
    "    training_data_whole = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "training_data = []\n",
    "for i in training_data_whole:\n",
    "    if i[0] in MEME_IDS:\n",
    "        training_data.append(i)\n",
    "print(f'Number of memes: {len(training_data)}')\n",
    "\n",
    "texts = []  \n",
    "labels_index = {}  \n",
    "labels = []  \n",
    "label_id_counter = 0\n",
    "\n",
    "for i, row in enumerate(training_data):\n",
    "    template_id = str(row[0]).zfill(12)\n",
    "    text = row[1].lower()\n",
    "    start_index = len(template_id) + 2 + 1 + 2  # template_id, spaces, box_index, spaces\n",
    "    box_index = 0\n",
    "    \n",
    "    for j in range(0, len(text)):\n",
    "        char = text[j]\n",
    "        # note: it is critical that the number of spaces plus len(box_index) is >= the convolution width\n",
    "        texts.append(template_id + '  ' + str(box_index) + '  ' + text[0:j])\n",
    "        if char in labels_index:\n",
    "            label_id = labels_index[char]\n",
    "        else:\n",
    "            label_id = label_id_counter\n",
    "            labels_index[char] = label_id\n",
    "            label_id_counter += 1\n",
    "        labels.append(label_id)\n",
    "        if char == '|':\n",
    "            box_index += 1\n",
    "\n",
    "    if i >= ROWS_TO_SCAN:\n",
    "        break\n",
    "print(f'Rows to process: {len(texts)}')\n",
    "\n",
    "del training_data\n",
    "\n",
    "# Use char to int mapping\n",
    "char_to_int = map_char_to_int(texts)\n",
    "sequences = texts_to_sequences(texts, char_to_int)\n",
    "\n",
    "del texts\n",
    "\n",
    "# dump the parameters into a json file\n",
    "with open(MODEL_PATH + '/params.json', 'w') as params:\n",
    "    json.dump({\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'num_rows_used': len(sequences),\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'char_to_int': char_to_int,\n",
    "        'labels_index': labels_index\n",
    "    }, params)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=SEQUENCE_LENGTH)\n",
    "del sequences  \n",
    "\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# train and validation split\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Use a 2% validation split size if rows are more than a million\n",
    "\n",
    "validation_ratio = 0.2 if data.shape[0] < (10**6) else 0.02\n",
    "num_validation_samples = int(validation_ratio * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "del data, labels\n",
    "\n",
    "print(f'Train Set Size: {x_train.shape[0]}')\n",
    "print(f'Validation Set Size: {x_val.shape[0]}')\n",
    "\n",
    "# Model Definition\n",
    "# with strategy.scope():\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_to_int) + 1, EMBEDDING_DIM, input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# make sure we only keep the weights from the epoch with the best accuracy, rather than the last set of weights\n",
    "checkpoint_handler = ModelCheckpoint(filepath=MODEL_PATH + '/model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=[checkpoint_handler])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
